{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDream: Direct Latent Optimization\n",
    "\n",
    "This notebook implements Direct Latent Optimization using CLIP. It optimizes image pixels to match a text prompt while preserving the original topology using Weighted Total Variation (TV) loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Normalize, ToTensor, ToPILImage, RandomAffine\n",
    "import open_clip\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "Using **ViT-H-14** (Vision Transformer Huge) for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT-H-14' \n",
    "pretrained = 'laion2b_s32b_b79k'\n",
    "print(f\"Loading {model_name}...\")\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device, precision='fp16')\n",
    "tokenizer = open_clip.get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Components\n",
    "Includes `WeightedTVLoss` for topology preservation and `MakeCutouts` for robust optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTVLoss(nn.Module):\n",
    "    def forward(self, img, weight_map):\n",
    "        h_diff = torch.pow((img[:,:,1:,:] - img[:,:,:-1,:]), 2)\n",
    "        w_diff = torch.pow((img[:,:,:,1:] - img[:,:,:,:-1]), 2)\n",
    "        \n",
    "        h_weight = weight_map[:,:,:-1,:]\n",
    "        w_weight = weight_map[:,:,:,:-1]\n",
    "\n",
    "        h_tv = (h_diff * h_weight).sum()\n",
    "        w_tv = (w_diff * w_weight).sum()\n",
    "        \n",
    "        return h_tv + w_tv\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.0):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.augs = torch.nn.Sequential(\n",
    "            RandomAffine(degrees=7, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        )\n",
    "        self.noise_fac = 0.05\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutout = F.interpolate(cutout, (self.cut_size, self.cut_size), mode='bilinear', align_corners=False)\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        cutouts = torch.cat(cutouts)\n",
    "        cutouts = self.augs(cutouts)\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = cutouts.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
    "            \n",
    "        return cutouts\n",
    "\n",
    "def create_detail_mask(image_pil, blur_radius=5, threshold=30, min=0):\n",
    "    img_cv = np.array(image_pil.convert('L'))\n",
    "    grad_x = cv2.Sobel(img_cv, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(img_cv, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    mask = magnitude - threshold\n",
    "    mask = mask / 50.0 + min\n",
    "    mask = np.clip(mask, 0, 1)\n",
    "    mask = mask.astype(np.float32)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    mask_pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "    mask_pil = mask_pil.filter(ImageFilter.GaussianBlur(blur_radius))\n",
    "    return ToTensor()(mask_pil).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_masked(image_path, text_prompt, \n",
    "                 negative_prompt=None, \n",
    "                 negative_weight=0.8,\n",
    "                 tv_weight=0.0002,\n",
    "                 octave_scales=[0.6, 1.2, 2.4, 3.8], \n",
    "                 steps_per_octave=70, \n",
    "                 learning_rate=0.01, \n",
    "                 num_cutouts=120, \n",
    "                 base_size=1024, \n",
    "                 cutout_batch_size=40,\n",
    "                 mask_blur=3,        \n",
    "                 mask_threshold=15,\n",
    "                 global_view_weight=0.03,\n",
    "                 gradient_blur_kernel_size=3):\n",
    "    \n",
    "    if image_path.startswith('http'):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(image_path, headers=headers, stream=True)\n",
    "        img_pil = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        img_pil = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Get CLIP Embeddings\n",
    "    text_inputs = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        target_embed = model.encode_text(text_inputs).detach()\n",
    "        target_embed = target_embed / target_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    neg_embed = None\n",
    "    if negative_prompt:\n",
    "        neg_inputs = tokenizer([negative_prompt]).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            neg_embed = model.encode_text(neg_inputs).detach()\n",
    "            neg_embed = neg_embed / neg_embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    current_tensor = None\n",
    "    tv_loss_fn = WeightedTVLoss()\n",
    "    \n",
    "    for idx, scale in enumerate(octave_scales):\n",
    "        new_width = int(base_size * scale)\n",
    "        new_height = int(base_size * scale)\n",
    "        print(f\"\\n--- Octave {idx+1}: {new_width}x{new_height} ---\")\n",
    "\n",
    "        if current_tensor is None:\n",
    "            img_resized = img_pil.resize((new_width, new_height), Image.LANCZOS)\n",
    "            img_tensor = ToTensor()(img_resized).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            img_tensor = F.interpolate(current_tensor, size=(new_height, new_width), mode='bicubic', align_corners=False)\n",
    "        \n",
    "        img_tensor = img_tensor.detach().requires_grad_(True)\n",
    "        optimizer = optim.Adam([img_tensor], lr=learning_rate)\n",
    "\n",
    "        # Generate Smoothness Map\n",
    "        current_pil = ToPILImage()(img_tensor.squeeze(0).cpu())\n",
    "        detail_mask = create_detail_mask(current_pil, blur_radius=mask_blur, threshold=mask_threshold)\n",
    "        smoothness_map = torch.pow(1.0 - detail_mask, 2) \n",
    "\n",
    "        make_cutouts = MakeCutouts(224, cutout_batch_size, cut_pow=1.0)\n",
    "\n",
    "        for i in range(steps_per_octave):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Global View\n",
    "            global_view = F.interpolate(img_tensor, (224, 224), mode='bilinear', align_corners=False)\n",
    "            global_norm = (global_view - torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device).view(1,3,1,1)) / \\\n",
    "                          torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device).view(1,3,1,1)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                global_embed = model.encode_image(global_norm)\n",
    "                global_embed = global_embed / global_embed.norm(dim=-1, keepdim=True)\n",
    "                global_dists = (global_embed @ target_embed.T).clamp(-0.999, 0.999)\n",
    "                global_loss = torch.acos(global_dists) * 2 / np.pi\n",
    "                global_loss = global_loss.mean()\n",
    "\n",
    "            (global_loss * global_view_weight).backward(retain_graph=True)\n",
    "\n",
    "            total_loss = 0\n",
    "            num_batches = num_cutouts // cutout_batch_size\n",
    "            \n",
    "            for _ in range(num_batches):\n",
    "                cutouts = make_cutouts(img_tensor)\n",
    "                cutouts_norm = (cutouts - torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device).view(1,3,1,1)) / \\\n",
    "                               torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device).view(1,3,1,1)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    image_embeds = model.encode_image(cutouts_norm)\n",
    "                    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    pos_sim = (target_embed @ image_embeds.T).mean()\n",
    "                    loss = 1 - pos_sim\n",
    "                    \n",
    "                    if neg_embed is not None:\n",
    "                        neg_sim = (neg_embed @ image_embeds.T).mean()\n",
    "                        loss = loss + (negative_weight * neg_sim)\n",
    "                    \n",
    "                    loss = loss + (tv_weight * tv_loss_fn(img_tensor, smoothness_map))\n",
    "\n",
    "                (loss / num_batches).backward()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if img_tensor.grad is not None:\n",
    "                img_tensor.grad = TF.gaussian_blur(img_tensor.grad, kernel_size=gradient_blur_kernel_size, sigma=0.5)\n",
    "                \n",
    "            if img_tensor.grad is not None:\n",
    "                g = img_tensor.grad\n",
    "                grad_contrast_pow = 1.5 \n",
    "                g_max = torch.abs(g).max() + 1e-8\n",
    "                g_norm = g / g_max\n",
    "                g_new = torch.sign(g_norm) * torch.abs(g_norm).pow(grad_contrast_pow)\n",
    "                img_tensor.grad = g_new * g_max\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_tensor.clamp_(0, 1)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Step {i} | Loss: {total_loss / num_batches:.4f}\")\n",
    "\n",
    "        current_tensor = img_tensor\n",
    "    \n",
    "    return current_tensor.cpu().detach().squeeze(0).permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "IMAGE_URL = \"images/forest_trail_1024.jpg\"\n",
    "PROMPT = \"Image representing Imagination - 8k colofull pattern\"\n",
    "NEG_PROMPT = \"text, noise, grain, blur, watermark, jpeg artifacts, low quality image, color noise, signature\"\n",
    "\n",
    "# Optimization Hyperparameters\n",
    "NEGATIVE_WEIGHT = 0.8\n",
    "TV_WEIGHT = 0.0002\n",
    "GLOBAL_VIEW_WEIGHT = 0.03\n",
    "\n",
    "# Resolution & Scale\n",
    "BASE_SIZE = 1024\n",
    "OCTAVE_SCALES = [0.6, 1.2, 2.4, 3.8]\n",
    "\n",
    "# Performance & Quality\n",
    "STEPS_PER_OCTAVE = 70 \n",
    "LEARNING_RATE = 0.01 \n",
    "NUM_CUTOUTS = 120\n",
    "CUTOUT_BATCH_SIZE = 40\n",
    "\n",
    "# Masking & Details\n",
    "MASK_THRESHOLD = 15\n",
    "MASK_BLUR = 3\n",
    "GRADIENT_BLUR_KERNEL_SIZE = 3\n",
    "\n",
    "# --- Run ---\n",
    "original = Image.open(IMAGE_URL).convert(\"RGB\")\n",
    "\n",
    "result = dream_masked(\n",
    "    IMAGE_URL, \n",
    "    PROMPT, \n",
    "    negative_prompt=NEG_PROMPT,\n",
    "    negative_weight=NEGATIVE_WEIGHT,\n",
    "    tv_weight=TV_WEIGHT,\n",
    "    octave_scales=OCTAVE_SCALES,\n",
    "    steps_per_octave=STEPS_PER_OCTAVE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_cutouts=NUM_CUTOUTS,\n",
    "    base_size=BASE_SIZE,\n",
    "    cutout_batch_size=CUTOUT_BATCH_SIZE,\n",
    "    mask_threshold=MASK_THRESHOLD,\n",
    "    mask_blur=MASK_BLUR,\n",
    "    global_view_weight=GLOBAL_VIEW_WEIGHT,\n",
    "    gradient_blur_kernel_size=GRADIENT_BLUR_KERNEL_SIZE\n",
    ")\n",
    "\n",
    "# --- Visualize ---\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(TF.resize(original, (BASE_SIZE, BASE_SIZE)))\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(result)\n",
    "plt.title(\"Dreamed\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Save ---\n",
    "if result.dtype != np.uint8:\n",
    "    result_to_save = np.clip(result * 255, 0, 255).astype(np.uint8)\n",
    "else:\n",
    "    result_to_save = result\n",
    "\n",
    "image = Image.fromarray(result_to_save)\n",
    "image.save(\"dream_result_fullres.png\")\n",
    "print(\"Saved full-resolution image as dream_result_fullres.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
